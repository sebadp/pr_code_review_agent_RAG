# ğŸ§  Local AI Code Reviewer
An intelligent code review assistant that runs locally using LangChain, Ollama, and ChromaDB â€” optimized for real pull requests and real codebases.

![screenshot or demo gif here]

---

## ğŸš€ Features
- ğŸ” **Semantic code search** with line-level references (file + line number)
- ğŸ’¬ **Local inference** using [Ollama](https://ollama.com/) + Mistral or Qwen
- ğŸ“¦ **RAG-powered insights** using LangChain + ChromaDB
- ğŸ§  **Memory support** via LangGraph for contextual conversations
- âœ… **PR-aware architecture** ready for GitHub integration
- ğŸ§¼ **Clean codebase** following service-oriented architecture

---

## ğŸ—ï¸ Architecture Overview
```
LocalRAG/
â”œâ”€â”€ .env
â”œâ”€â”€ .venv/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Makefile
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ client.py
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ routers/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py
â”‚   â”‚   â”‚   â””â”€â”€ github.py
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ github/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ pr_loader.py
â”‚   â”‚   â”œâ”€â”€ langchain/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ chains.py
â”‚   â”‚   â”‚   â””â”€â”€ prompts.py
â”‚   â”‚   â””â”€â”€ vectorstore/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â””â”€â”€ chroma.py
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ schemas.py
â”œâ”€â”€ data/
â”‚   â””â”€â”€ vectorstore/

```

### Services
- VectorStoreService: Handles indexing and retrieval
- OllamaService: Local LLM client
- GitHubService: Pull request integration
- Memory: LangGraph nodes to manage chat history

---

## ğŸ› ï¸ Tech Stack
- **LangChain** â€” RAG, memory, embedding abstraction
- **ChromaDB** â€” Vector database
- **Ollama** â€” Local LLM inference (Mistral, Qwen, etc.)
- **LangGraph** â€” Memory-aware multi-step conversations
- **FastAPI** â€” Lightweight API server
- **Python 3.10+**

---

## ğŸ“¥ Getting Started

### 1. Clone & Install
```bash
git clone https://github.com/yourusername/local-ai-code-reviewer.git
cd local-ai-code-reviewer
pip install -r requirements.txt
```

### 2. Start Ollama
```bash
ollama run mistral
```
Or use:
```bash
ollama run qwen:7b
```

### 3. Embed Your Repo
```bash
python ingest_repo.py
```
*(Make sure your repo is inside the `repos/` folder)*

### 4. Launch the API
```bash
uvicorn chat_api:app --reload
```

### 5. Ask Questions
```bash
python client.py
```

## ğŸ“ Referencing Code with File + Line
Each code chunk is enriched with metadata during ingestion:
```
// File: chat_api.py - Line: 47
@app.post("/ask")
async def ask_question(request: QuestionRequest):
```
This allows the agent to ground its feedback to the exact place in the code.

## ğŸ”® Coming Soon
* Web frontend (Streamlit or FastAPI + React)
* Auto-suggested review comments
* LLM self-evaluation for feedback quality
* Multi-agent code review (naming, perf, security, etc.)

## ğŸ¤ Contributing
Contributions welcome! Feel free to open issues, PRs, or reach out on LinkedIn.

## ğŸ“œ License
This project is licensed under the MIT License.

## ğŸ“§ Contact
[linkedin](https://www.linkedin.com/in/sebastiandavila/)
